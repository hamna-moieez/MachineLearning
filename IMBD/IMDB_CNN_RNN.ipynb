{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_CNN_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSKWs7jChKmw",
        "colab_type": "text"
      },
      "source": [
        "# **HOMEWORK - 5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJZdK8GDhRw5",
        "colab_type": "text"
      },
      "source": [
        "## **1. IMDB dataset**\n",
        "\n",
        "\n",
        "*   Max Features = 5000\n",
        "*   Max Length = 250 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ccnNZtLh_wp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "7de1ebfa-6202-4e7f-ee16-2dc01e89f4d6"
      },
      "source": [
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.datasets import imdb\n",
        "\n",
        "# define params\n",
        "max_features = 5000\n",
        "maxlen = 400\n",
        "batch_size = 32\n",
        "embedding_dims = 50\n",
        "filters = 250\n",
        "kernel_size = 3\n",
        "hidden_dims = 250\n",
        "epochs = 2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94cVh9eoiIkh",
        "colab_type": "text"
      },
      "source": [
        "### **Load Dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHlKJKmliURF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "5f8df74e-2846-4048-9edc-2aca7c5cee32"
      },
      "source": [
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 3s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozr4XEdSiWKp",
        "colab_type": "text"
      },
      "source": [
        "### **Pad Samples:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "707pOPLAihmi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "97634cd5-7c08-48ea-9145-613ab3b7bd90"
      },
      "source": [
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 400)\n",
            "x_test shape: (25000, 400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxiUU4j9irlh",
        "colab_type": "text"
      },
      "source": [
        "# **1-D CNN Model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNimHOgEEMIs",
        "colab_type": "code",
        "outputId": "d24c44b2-7718-42ed-c3bd-e5da02bab6ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        }
      },
      "source": [
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "model.add(Embedding(max_features,\n",
        "                    embedding_dims,\n",
        "                    input_length=maxlen))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# we add a Convolution1D, which will learn filters\n",
        "# word group filters of size filter_length:\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "# we use max pooling:\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/2\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "25000/25000 [==============================] - 14s 547us/step - loss: 0.4133 - acc: 0.7946 - val_loss: 0.2737 - val_acc: 0.8860\n",
            "Epoch 2/2\n",
            "25000/25000 [==============================] - 6s 227us/step - loss: 0.2323 - acc: 0.9075 - val_loss: 0.2649 - val_acc: 0.8907\n",
            "25000/25000 [==============================] - 1s 49us/step\n",
            "Test score: 0.2648929985904694\n",
            "Test accuracy: 0.89068\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GWFlkTFh2Re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTBoj2-wgVPw",
        "colab_type": "text"
      },
      "source": [
        "# **Build LSTM Model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNHnRNBORebc",
        "colab_type": "code",
        "outputId": "638a55b7-807d-4b2c-c544-f2d9cfe5c094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "from keras.layers import LSTM\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Train...\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/2\n",
            "25000/25000 [==============================] - 651s 26ms/step - loss: 0.4685 - acc: 0.7812 - val_loss: 0.3981 - val_acc: 0.8278\n",
            "Epoch 2/2\n",
            "25000/25000 [==============================] - 646s 26ms/step - loss: 0.3666 - acc: 0.8441 - val_loss: 0.3617 - val_acc: 0.8465\n",
            "25000/25000 [==============================] - 117s 5ms/step\n",
            "Test score: 0.36169198896408083\n",
            "Test accuracy: 0.84652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcfjxdvCjmqe",
        "colab_type": "text"
      },
      "source": [
        "# **Build CNN and RNN Combination Model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5PhGEH5W3zp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "b7234d95-397c-42ce-d17b-9a678025df43"
      },
      "source": [
        "# Embedding\n",
        "max_features = 5000\n",
        "maxlen = 250\n",
        "embedding_size = 128\n",
        "\n",
        "# Convolution\n",
        "kernel_size = 5\n",
        "filters = 64\n",
        "pool_size = 4\n",
        "\n",
        "# LSTM\n",
        "lstm_output_size = 70\n",
        "\n",
        "# Training\n",
        "batch_size = 30\n",
        "epochs = 2\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(MaxPooling1D(pool_size=pool_size))\n",
        "model.add(LSTM(lstm_output_size))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 250)\n",
            "x_test shape: (25000, 250)\n",
            "Build model...\n",
            "Train...\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/2\n",
            "25000/25000 [==============================] - 126s 5ms/step - loss: 0.3678 - acc: 0.8256 - val_loss: 0.3327 - val_acc: 0.8635\n",
            "Epoch 2/2\n",
            "25000/25000 [==============================] - 124s 5ms/step - loss: 0.2281 - acc: 0.9108 - val_loss: 0.2976 - val_acc: 0.8796\n",
            "25000/25000 [==============================] - 35s 1ms/step\n",
            "Test score: 0.2976207625463605\n",
            "Test accuracy: 0.8795999918937684\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}